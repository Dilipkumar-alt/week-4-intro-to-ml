# Next Word Predictor using Transformers (GPT-2)

This project is the final wrap-up of a 4-week NLP series. It demonstrates how to build and fine-tune a next-word prediction model using the GPT-2 transformer architecture. It leverages Hugging Face's `transformers` and `datasets` libraries.

## 🚀 Features

- Fine-tunes GPT-2 on the WikiText-2 dataset
- Evaluates using perplexity and top-k accuracy
- Interactive text prediction using Gradio
- Clean modular scripts for training, evaluation, and inference

## 📁 Project Structure

